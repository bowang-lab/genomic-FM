from __future__ import annotations

from copy import deepcopy
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
from torch.utils.data import Dataset

from .olmo_utils.util import file_size, get_bytes_range
from .olmo_utils.aliases import PathOrStr
__all__ = ["MemMapDataset"]


class MemMapDataset(Dataset[Dict[str, Any]]):
    """
    A PyTorch :class:`~torch.utils.data.Dataset` backed by one or more numpy memory-mapped arrays
    of token IDs. Token IDs are chunked together into contiguous blocks of ``chunk_size``
    to create instances.

    If the length of a memory-mapped array is not a multiple of ``chunk_size`` the
    remainder of the tokens will be ignored.

    No special tokens are added to the input IDs so it's assumed that if you want
    EOS tokens between documents, for example, those will already be in the memory-mapped array.

    :param paths: Paths to memory-mapped token arrays.
    :param chunk_size: The number of tokens to chunk together into a single instance.
        Generally this should correspond to your model's maximum input length.
    :param memmap_dtype: The numpy datatype of the memory-mapped array.
    :param metadata: Metadata to add to each item. This should be a dictionary or a list of dictionaries
        with the same number of items as there are paths.
    :param include_instance_metadata: If ``True`` (the default), each instance returned from `__getitem__` will
        include the metadata from its source.
    :param generate_attention_mask: If ``True``, each instance returned from ``__getitem__`` will include an
        attention mask generated by masking each padding token.
    :param pad_token_id: The ID of the padding token. Required if ``generate_attention_mask`` is ``True``.
    :param label_mask_paths: Optional paths to ``np.bool_`` memory-mapped arrays of label masks.
    """

    def __init__(
        self,
        path_seq1: List[PathOrStr],
        path_seq2: List[PathOrStr],
        seq_shape: Tuple[int, int],
        chunk_size: int = 1024,
        memmap_dtype=np.float32,
        metadata: Optional[Union[List[Dict[str, Any]], Dict[str, Any]]] = None,
        include_instance_metadata: bool = True,
        generate_attention_mask: bool = False,
        pad_token_id: Optional[int] = None,
        label_mask_paths: Optional[List[PathOrStr]] = None,
        annotation_paths: Optional[List[PathOrStr]] = None,
        label_paths: Optional[List[PathOrStr]] = None,
        annot_dtype: Optional[np.dtype] = np.int64,
        label_dtype: Optional[np.dtype] = np.int64,

    ):
        if not path_seq1 or not path_seq2 or not annotation_paths:
            raise ValueError("path_seq1, path_seq2, and label_and_annotation_paths are required")

        if len(path_seq1) != len(annotation_paths) or len(path_seq1) != len(path_seq2) or len(path_seq2) != len(annotation_paths):
            raise ValueError("The number of paths must be the same for path_seq1, path_seq2, and label_and_annotation_paths")

        self._memmap_paths_seq1 = path_seq1
        self._memmap_paths_seq2 = path_seq2
        self._annotation_paths = annotation_paths
        self._label_paths = label_paths

        self.dtype = memmap_dtype
        self._annot_dtype = annot_dtype
        self._label_dtype = label_dtype
        self.shape = seq_shape

        self._metadata = metadata
        # chunk size is the multiplication of every dimension in the shape
        self._chunk_size = seq_shape[0] * seq_shape[1]
        self._mmap_offsets: Optional[List[Tuple[int, int]]] = None
        self._num_instances: Optional[int] = None
        self._include_instance_metadata = include_instance_metadata
        self._generate_attention_mask = generate_attention_mask
        self._pad_token_id = pad_token_id

    @property
    def chunk_size(self) -> int:
        return self._chunk_size

    @property
    def max_seq_len(self) -> int:
        # For compatibility with composer's SpeedMonitor callback.
        return self.chunk_size

    @property
    def offsets(self) -> List[Tuple[int, int]]:
        if self._mmap_offsets is None:
            import concurrent.futures

            self._mmap_offsets = []

            path_to_seq2_path: Dict[PathOrStr, PathOrStr] = {}
            path_to_annotations_path: Dict[PathOrStr, PathOrStr] = {}
            path_to_label_path: Dict[PathOrStr, PathOrStr] = {}

            seq1_path_to_length: Dict[PathOrStr, int] = {}
            seq2_path_to_length: Dict[PathOrStr, int] = {}
            annotations_path_to_length: Dict[PathOrStr, int] = {}
            label_path_to_length: Dict[PathOrStr, int] = {}

            with concurrent.futures.ThreadPoolExecutor() as executor:
                seq1_path_futures = []
                seq2_path_futures = []
                annotations_path_futures = []
                label_path_futures = []
                for i, path in enumerate(self._memmap_paths_seq1):
                    seq1_path = self._memmap_paths_seq1[i]
                    seq2_path = self._memmap_paths_seq2[i]
                    annotations_path = self._annotation_paths[i]
                    label_path = self._label_paths[i]
                    path_to_seq2_path[seq1_path] = seq2_path
                    path_to_annotations_path[seq1_path] = annotations_path
                    path_to_label_path[seq1_path] = label_path
                    seq1_path_futures.append(executor.submit(self._get_file_length, seq1_path, dtype=self.dtype))
                    seq2_path_futures.append(executor.submit(self._get_file_length, seq2_path, dtype=self.dtype))
                    annotations_path_futures.append(executor.submit(self._get_file_length, annotations_path, is_label=True, dtype=self._annot_dtype))
                    label_path_futures.append(executor.submit(self._get_file_length, label_path, is_label=True, dtype=self._label_dtype))

                for future in concurrent.futures.as_completed(seq1_path_futures):
                    path, length = future.result()
                    seq1_path_to_length[path] = length

                for future in concurrent.futures.as_completed(seq2_path_futures):
                    path, length = future.result()
                    seq2_path_to_length[path] = length

                for future in concurrent.futures.as_completed(annotations_path_futures):
                    path, length = future.result()
                    annotations_path_to_length[path] = length

                for future in concurrent.futures.as_completed(label_path_futures):
                    path, length = future.result()
                    label_path_to_length[path] = length

            start_offset = 0
            for path in self._memmap_paths_seq1:
                length = seq1_path_to_length[path]
                end_offset = start_offset + length
                self._mmap_offsets.append((start_offset, end_offset))
                start_offset += length
                if seq2_path_to_length:
                    seq2_path = path_to_seq2_path[path]
                    if length != seq2_path_to_length[seq2_path]:
                        raise ValueError(f"seq2 file '{seq2_path}' should be the same size as '{path}'")
                if annotations_path_to_length:
                    annotations_path = path_to_annotations_path[path]
                    if length != annotations_path_to_length[annotations_path]:
                        raise ValueError(f"annotations file '{annotations_path}' should be the same size as '{path}' length {length} != {annotations_path_to_length[annotations_path]}")
                if label_path_to_length:
                    label_path = path_to_label_path[path]
                    if length != label_path_to_length[label_path]:
                        raise ValueError(f"label file '{label_path}' should be the same size as '{path}'")
        return self._mmap_offsets

    def _read_chunk_from_memmap(self, path: PathOrStr, index: int, dtype=None,
                                is_label=False) -> torch.Tensor:
        chunk_size = self._chunk_size
        if is_label:
            chunk_size = 1
        dtype = dtype
        item_size = dtype(0).itemsize
        bytes_start = index * item_size * chunk_size
        num_bytes = item_size * chunk_size
        buffer = get_bytes_range(path, bytes_start, num_bytes)
        array = np.frombuffer(buffer, dtype=dtype)
        if dtype == np.float32 and is_label==False:
            return torch.tensor(array.reshape(self.shape), dtype=torch.float32)
        elif dtype == np.float32 and is_label==True:
            return torch.tensor(array.astype(np.float32), dtype=torch.float32)
        elif dtype == np.int64 and is_label==True:
            return torch.tensor(array.astype(np.int64), dtype=torch.long)

    def _get_file_length(self, path, dtype=None, is_label=False) -> Tuple[PathOrStr, int]:
        chunk_size = self._chunk_size
        # multiple by every dimension in the shape
        if is_label:
            chunk_size = 1
        dtype = dtype
        item_size = dtype(0).itemsize
        print(f"args: {path}, {dtype}, {is_label}, {chunk_size}")
        print(f"file_size: {file_size(path)}")
        print(f"item_size: {item_size}")
        return path, file_size(path) // (item_size * chunk_size)

    def __len__(self) -> int:
        if self._num_instances is None:
            self._num_instances = self.offsets[-1][1]
        return self._num_instances

    def __getitem__(self, index: int) -> Dict[str, Any]:
        index = int(index)  # in case this is a numpy int type.
        pos_index = index if index >= 0 else len(self) + index

        # The index of the memmap array within 'self.memmaps'
        memmap_index: Optional[int] = None
        # The 'index' relative to the corresponding memmap array.
        memmap_local_index: Optional[int] = None
        for i, (offset_start, offset_end) in enumerate(self.offsets):
            if offset_start <= pos_index < offset_end:
                memmap_index = i
                memmap_local_index = pos_index - offset_start

        if memmap_index is None or memmap_local_index is None:
            raise IndexError(f"{index} is out of bounds for dataset of size {len(self)}")

        seq1 = self._read_chunk_from_memmap(self._memmap_paths_seq1[memmap_index], memmap_local_index,
                                            dtype=self.dtype)
        # out = {"seq1": seq1}

        seq2 = self._read_chunk_from_memmap(self._memmap_paths_seq2[memmap_index], memmap_local_index,
                                            dtype=self.dtype)
        # out["seq2"] = seq2

        annot = self._read_chunk_from_memmap(self._annotation_paths[memmap_index], memmap_local_index,
                                                         dtype=self._annot_dtype, is_label=True)

        label = self._read_chunk_from_memmap(self._label_paths[memmap_index], memmap_local_index,
                                                    dtype=self._label_dtype, is_label=True)
        return [[seq1, seq2, annot], label]

    def __add__(self, other: MemMapDataset) -> MemMapDataset:
        """
        Concatenate one :class:`MemMapDataset` with another.
        """
        if not isinstance(other, MemMapDataset):
            raise NotImplementedError(f"Expected another MemMapDataset but got {type(other)}")
        new_seq1_paths = self._memmap_paths_seq1 + other._memmap_paths_seq1
        new_seq2_paths = self._memmap_paths_seq2 + other._memmap_paths_seq2
        new_annot = self._annotation_paths + other._annotation_paths
        new_label = self._label_paths + other._label_paths
        return MemMapDataset(
            path_seq1=new_seq1_paths,
            path_seq2=new_seq2_paths,
            chunk_size=self._chunk_size,
            memmap_dtype=self.dtype,
            metadata=self._metadata + other._metadata,
            annotation_paths=new_annot,
            label_paths=new_label,
        )
